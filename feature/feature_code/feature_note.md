# 该部分主要介绍特征工程
## 什么是特征工程？
简单说，就是发现对因变量y有明显影响作用的特征，通常称自变量x为特征，特征工程的目的是发现重要特征。
一般来说，特征工程大体上可以分为三个方面，
* 特征构造
    特征构造。顾名思义，特征构造就是构造出来新的特征，一般都是通过对原有的特征进行四则运算产生新的特征。
    比如原来的特征是x1和x2，那么x1+x2就是一个新的特征，或者当x1大于某个数c的时候，就产生一个新的变量x3，并且x3=1，当x1小于c的时候，x3=0，
    所以这样看来呢，可以按照这种方法构造出很多特征，这个就是构造。
    
    
* 特征选择
    ** 特征选择是非常重要的。**
    如果特征选择失败，那么前面的工作都是徒劳的，大家试想一下，比如我们构造出了100多个特征，却不知道哪个特征最重要，那不是还是没有解决问题吗，所以特征选择就显的尤为重要。
    我们首先对特征选择的方法进行一个分类，从机器学习和统计方面进行一个分类，看看各自有哪些常用的算法。
    统计的角度：
        回归分析，比如自变量x的回归系数如果是不显著的，那么我们就认为它不是一个好的特征。
        还有常用的手段就是，先把所有的变量一起和y做一个回归，在逐渐去掉某些x，观察模型的deviance如何变化，这类方法其实叫 Wrapper Methods。
        但是呢，如果因变量y是离散的，那么我们的回归对应的就是广义线性回归模型了，
        那么这个时候还可以通过其他方法进行特征选择了，一般做法是单独将每个x和y做一个回归，完后后再计算这个模型对应的基尼系数，
    通过观察基尼系数确定哪个方程对应的特征是最好的，但是呢，这种方法存在缺点，一个是每一个变量都需要单独和y做回归，比较麻烦
    。第二个是按照这种方法选出来的特征，其实效果不行，也就是在测试集上面表现并不好。那么呢，难道统计学方面就没有好一点方法吗
    ，有呢，就是lasso回归，lasso回归本来就是统计学大师搞出来的，现在已经发展出好多lasso回归的模型了。那么大家一看lasso这个单词是套索的意思。
    就是套着马脖子，主要是控制马的装置，那么在回归当中呢，主要是套回归系数的，意思是把回归系数控制住，不让它太大，本质上是一种正则化的手段，
    但是呢，不叫它太大，那数学的语言刻画方式有很多种了，但是lasso回归就选择了很巧妙的一种刻画方式，按照这种套路来，不仅可以约束系数，
    而且可以在模型最优的时候把不重要的系数约束为0.既然x的系数都成了0了，那我们也不用看显著性了，直接做到了特征选择，或者变量选择。
    目前看，这个方法是效果是非常好的，而且非常实用与高维数据分析。说的官方一些，这类方法叫Embedded Methods，后面我们会专门讲lasso。上面就是统计学角度的变量选择。
    机器学习角度：
    回归数，或者叫决策树。模型本身可解释性很强。它是按照x的值，对y进行了划分，划分的好坏依据是啥呢，主要是纯度，一个划分块里，
纯度高，就说明划分的好，也就说明了这个划分变量选择的好。变量的重要性也就不言而喻了。当然这里是个简单的介绍，后面会详细地解读。
实际上除了决策树，随机深林（random forest），bagging，boosting，gradient booting, 已经xgboost都有特征选择的功能，特别是xgboost，
这个算法很厉害，和BGM其实比较相似，后面我们会进行对比。除了基于树的模型呢，还有其他模型，比如神经网络，支持向量机也都有变量选择的功能。   
    
    
    
    
* 特征生成。
    特征生成，特征生成和特征构造有些相似，存在一些区别的，特征生成的话，比如主成分分析或者因子分析，
    都可以对原有数据进行特征生成，比如卷积神经网络的卷积层就是一个特征生成的过程，一张图片，经过卷积的不断扫描，
    就会把原始图片里边的好多特征逐步提取出来，这个就是特征生成。再如通过主成分分析对原始数据进行降维，
    那么降维后，每个主成分其实就代表一个新的特征，同样因子分析也具有这方面的功能，通过因子分析可以把潜在变量后面的潜在因子找出来。
    大家可以逐渐感觉到特征构造和特征生成会有一些区别，但是也有一定的联系。实际上，主成分分析本身就是初始变量的线性组合。
    这样来看，也属于特征构造的，但是一般特征构造就是简单的四则运算。



特征生成和特征构造，其实就是想尽各种办法产生个新的特征





好，那我们今天就到这，明天继续